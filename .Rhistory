color = 'black', lwd = 0.5)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
#geom_abline(slope = -W[2]/W[3], intercept = -W[1]/W[3],
geom_abline(slope = 1, intercept = -1,
color = 'black', lwd = 0.5)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
#geom_abline(slope = -W[2]/W[3], intercept = -W[1]/W[3],
geom_abline(slope = 0.5, intercept = -1,
color = 'black', lwd = 0.5)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
geom_abline(slope = -W[2]/W[3], intercept = -W[1]/W[3],
# geom_abline(slope = 0.5, intercept = -1,
color = 'black', lwd = 0.5)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
#geom_abline(slope = -W[2]/W[3], intercept = -W[1]/W[3],
geom_abline(slope = -1, intercept = -1,
color = 'black', lwd = 0.5)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
#geom_abline(slope = -W[2]/W[3], intercept = -W[1]/W[3],
geom_abline(slope = -1/2, intercept = -1,
color = 'black', lwd = 0.5)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
#geom_abline(slope = -W[2]/W[3], intercept = -W[1]/W[3],
geom_abline(slope = -3, intercept = 2,
color = 'black', lwd = 0.5)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,1,5, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,1,10, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,1,0.25, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,-5,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,-2,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,-1.6,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,-1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-1,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-1,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(1,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(0.8,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(0.5,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(0.2,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
#optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
optimizing_function <- function(x) ((-0.2) + (0.9)*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(0.2,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
#optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
optimizing_function <- function(x) ((1) + (0.9)*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(0.2,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
#optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
optimizing_function <- function(x) ((1) + (-0.9)*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(0.2,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
#optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
optimizing_function <- function(x) ((1) + (-1)*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(0.2,-0.9,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
for (i in 1:number_of_iterations){
W <- W - (step_size)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
#optimizing_function <- function(x) ((1) + (-1)*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve so far
}
# Examples of function above
optimizing(-1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-1,1,1, 0.01, 50)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-1,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-2,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-3,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-4,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-5,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-6,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-7,1,1, 0.01, 0)
#optimizing(1,1,1, 0.01, 1000)
#optimizing(1,1,-1, 0.01, 30000)
# Examples of function above
optimizing(-8,1,1, 0.01, 10)
optimizing(-8,1,1, 0.01, 1000)
optimizing(-8,1,1, 0.01, 30000)
# Examples of function above
optimizing(-8,1,1, 0.02, 10)
optimizing(-8,1,1, 0.02, 1000)
optimizing(-8,1,1, 0.02, 40000)
# Examples of function above
optimizing(-8,1,1, 0.05, 10)
optimizing(-8,1,1, 0.05, 1000)
optimizing(-8,1,1, 0.05, 40000)
# Examples of function above
optimizing(-8,1,1, 0.05, 10)
optimizing(-8,1,1, 0.05, 1000)
optimizing(-8,1,1, 0.05, 60000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
mse <- c(0,number_of_iterations)
iteration <- c(0, number_of_iterations)
for (i in 1:number_of_iterations){
mse_at_i <- gradMSE(X_data,y_data,W[1],W[2],W[3])
mse[i] <- mse_at_i
iteration[i] <- i
W <- W - (step_size)*mse_at_i
}
# Make a new data frame to store results for the learning curve
learning_curve_data <- cbind(mse, iteration)
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
#optimizing_function <- function(x) ((1) + (-1)*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve
optimizing_curve <- ggplot(learning_curve_data, aes(x = iteration, y = mse)) +
geom_point(size = 1.2, col = 'darkgreen', shape = 2) +
geom_line(lwd = 1.2, col = 'darkgreen') + ylab('Mean Squared Error') +
ggtitle(sprintf('Learning Curve with step: %0.4f', step_size)) +
theme_classic(12)
optimizing_curve
}
# Examples of function above
optimizing(-8,1,1, 0.05, 10)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
mse <- c(0,number_of_iterations)
iteration <- c(0, number_of_iterations)
for (i in 1:number_of_iterations){
mse_at_i <- gradMSE(X_data,y_data,W[1],W[2],W[3])
mse[i] <- mse_at_i
iteration[i] <- i
W <- W - (step_size)*mse_at_i
}
# Make a new data frame to store results for the learning curve
learning_curve_data <- cbind(mse, iteration)
learning_curve_data <- as.data.frame(learning_curve_data)
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve
optimizing_curve <- ggplot(learning_curve_data, aes(x = iteration, y = mse)) +
geom_point(size = 1.2, col = 'darkgreen', shape = 2) +
geom_line(lwd = 1.2, col = 'darkgreen') + ylab('Mean Squared Error') +
ggtitle(sprintf('Learning Curve with step: %0.4f', step_size)) +
theme_classic(12)
optimizing_curve
}
# Examples of function above
optimizing(-8,1,1, 0.05, 10)
optimizing(-8,1,1, 0.05, 1000)
#optimizing(-8,1,1, 0.05, 60000)
optimizing <- function(w0, w1, w2, step_size, number_of_iterations) {
W <- c(w0,w1,w2)
mse <- c(0,number_of_iterations + 1)
iteration <- c(0, number_of_iterations + 1)
for (i in 1:number_of_iterations){
mse_at_i <- gradMSE(X_data,y_data,W[1],W[2],W[3])
mse[i] <- mse_at_i
iteration[i] <- i
W <- W - (step_size)*mse_at_i
}
# Make a new data frame to store results for the learning curve
learning_curve_data <- cbind(mse, iteration)
learning_curve_data <- as.data.frame(learning_curve_data)
# Lets print the constants
print(W)
# Lets print the MSE at this point as well
print(paste("MSE: ",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
optimizing_function <- function(x) ((-W[1]/W[3]) + (-W[2]/W[3])*x)
# Current decision boundary location overlayed on the data
optimizing_plot <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal MSE: Petal Width vs Petal Length for Versicolor and Virginica") +
stat_function(fun = optimizing_function)
optimizing_plot
# Learning curve
optimizing_curve <- ggplot(learning_curve_data, aes(x = iteration, y = mse)) +
geom_point(size = 1.2, col = 'darkgreen', shape = 2) +
geom_line(lwd = 1.2, col = 'darkgreen') + ylab('Mean Squared Error') +
ggtitle(sprintf('Learning Curve with step: %0.4f', step_size)) +
theme_classic(12)
optimizing_curve
}
# Examples of function above
optimizing(-8,1,1, 0.05, 10)
optimizing(-8,1,1, 0.05, 1000)
optimizing(-8,1,1, 0.05, 60000)
