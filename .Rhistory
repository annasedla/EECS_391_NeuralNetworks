p
graph_data <- iris_data %>% filter(species %in% c('versicolor','virginica'))
# We are going to use the plot 3D library to graph this
x <- c(1,2,3,4,5,6,7,8,9,10)
y <- c(1,2,3,4,5,6,7,8,9,10)
z <- rep(0,10)
for (i in 1:10)
{
z[i]<-1/(1+exp(-(2.8 - 1/4*x[i] - y[i])))
}
p <- plot_ly(x = x, y = y, z = z) %>% add_surface()
p
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(datasets)
# Optimal Error from Optimization Algorithm
paste0("Optimal Mean Squared Error: ", mean_squared_error(X_data, y_data, -4.718, -0.04879, 3.0418))
gradMSE <- function(X, y, w0, w1, w2) {
dMSE_dw0 <- 0
dMSE_dw1 <- 0
dMSE_dw2 <- 0
m = nrow(X)
# Loop through all the examples in the input data
for (i in 1:m){
x_i1 = X$petal_length[i]
x_i2 = X$petal_width[i]
y_i = y$label[i]
sigma = one_layer_neural_network(w0, w1, w2, x_i1, x_i2)  # Output of neural network for example i
# Append the contributions to the sums for each gradient component
dMSE_dw0 <- dMSE_dw0 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(1)
dMSE_dw1 <- dMSE_dw1 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(x_i1)
dMSE_dw2 <- dMSE_dw2 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(x_i2)
}
# Compile all components into a 3-dimensional vector
gradient_result = c(dMSE_dw0, dMSE_dw1, dMSE_dw2)
# Return result as a vector
return(gradient_result)
}
# W vector
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:500){
# Step size
theta <- theta - (0.5)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
}
# W vector
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:500){
# Step size
theta <- theta - (0.5)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
}
# Input feature vectors
X_data <- iris_data %>% filter(species %in% c("versicolor","virginica")) %>% select(petal_length, petal_width)
# Pattern classes for each feature vector above
y_data <- iris_data %>%
filter(species %in% c("versicolor","virginica")) %>%
select(species) %>%
mutate(label = ifelse(species == "versicolor", 0, 1)) %>%
select(-species)
mean_squared_error <- function(X, y, w0, w1, w2) {
# Outputs of neural network for each example
NN_outputs <- rep(0,nrow(X))
for (i in 1:nrow(X)){
NN_outputs[i] = one_layer_neural_network(w0, w1, w2, X$petal_length[i], X$petal_width[i])
}
# Compute the differences between the NN outputs and the labels
result = mean((NN_outputs - y$label)^2)
# Return result
return(result)
}
# W vector
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:500){
# Step size
theta <- theta - (0.5)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
}
# Optimal Error from Optimization Algorithm
paste0("Optimal Mean Squared Error: ", mean_squared_error(X_data, y_data, -4.718, -0.04879, 3.0418))
z_function3 <- function(x) ((4.718/3.0148) + (0.04879/3.0418)*x)
plot4 <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal Mean Square Error: Petal Width vs Petal Length for Versicolor and Virginica") + stat_function(fun = z_function3)
plot4
# Versicolor
data1 <- iris_data %>% filter (species == 'versicolor')
for (i in 1:50){
if (one_layer_neural_network(-2.8, 0.25, 1, data1$petal_length[i], data1$petal_width[i]) < 0.5){
resulting_flower <- "versicolor"
} else {
resulting_flower <- "virginica"
}
print(paste("Neural network output for example", i,"=",one_layer_neural_network(-2.8, 0.25, 1, data1$petal_length[i], data1$petal_width[i])))
print(paste0("actual: versicolor, predicted: ", resulting_flower))
print(" ")
}
# Virginica
data2 <- iris_data %>% filter (species == 'virginica')
for (j in 1:50){
if (one_layer_neural_network(-2.8, 0.25, 1, data2$petal_length[j], data2$petal_width[j]) < 0.5){
resulting_flower <- "versicolor"
} else {
resulting_flower <- "virginica"
}
print(paste("Neural network output for example", j,"=",one_layer_neural_network(-2.8, 0.25, 1, data2$petal_length[j], data2$petal_width[j])))
print(paste0("actual: virginica, predicted: ", resulting_flower))
print(" ")
}
z_function <- function(x) (2.8 + (-1/4)*x)
# Lets pick the 2nd and 3rd classes: versicolor and virginica and let's plot the subset
plot2 <- iris_data %>% filter(species %in% c('versicolor','virginica')) %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Petal Width vs Petal Length for Versicolor and Virginica") + stat_function(fun = z_function)
plot2
# Initially constants are set to zero
W <- c(0,0,0)
# We are going to iterate five times and print the respective plots
for (i in 1:5){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
# Initially constants are set to zero
W <- c(0,0,0)
# We are going to iterate five times and print the respective plots
for (i in 1:3){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
# Initially constants are set to zero
W <- c(-6, 0.5 ,1)
# We are going to iterate five times and print the respective plots
for (i in 1:3){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
# Initially constants are set to zero
W <- c(-6, 0.5, 1)
# We are going to iterate five times and print the respective plots
for (i in 1:3){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
# Initially constants are set to zero
W <- c(0, 0, 0)
# We are going to iterate five times and print the respective plots
for (i in 1:3){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
# Initially constants are set to zero
W <- c(0, 0, 0)
# We are going to iterate five times and print the respective plots
for (i in 1:40){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
# Initially constants are set to zero
W <- c(0, 0, 0)
# We are going to iterate five times and print the respective plots
for (i in 1:40){
# Step size
W <- W - (0.005)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
# Initially constants are set to zero
W <- c(0, 0, 0)
# We are going to iterate five times and print the respective plots
for (i in 1:40){
# Step size
W <- W - (0.005)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
# Initially constants are set to zero
W <- c(0, 0, 0)
print(W)
print(paste("MSE Before Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# We are going to iterate five times and print the respective plots
for (i in 1:10000){
# Step size
W <- W - (0.005)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Initially constants are set to zero
W <- c(0, 0, 0)
print(W)
print(paste("MSE Before Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# We are going to iterate five times and print the respective plots
for (i in 1:20000){
# Step size
W <- W - (0.05)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Initially constants are set to zero
W <- c(0, 0, 0)
print(W)
print(paste("MSE Before Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# We are going to iterate five times and print the respective plots
for (i in 1:30000){
# Step size
W <- W - (0.005)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Initially constants are set to zero
W <- c(0, 0, 0)
print(W)
print(paste("MSE Before Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# We are going to iterate five times and print the respective plots
for (i in 1:10000){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Initially constants are set to zero
W <- c(0, 0, 0)
print(W)
print(paste("MSE Before Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# We are going to iterate five times and print the respective plots
for (i in 1:10000){
# Step size
W <- W - (0.01)*gradMSE(X_data,y_data,W[1],W[2],W[3])
}
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Initially constants are set to zero
W <- c(0, 0, 0)
print(W)
print(paste("MSE Before Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# We are going to iterate five times and print the respective plots
for (i in 1:1000){
# Step size
W <- W - (0.01)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# Initially constants are set to zero
W <- c(0, 0, 0)
print(W)
print(paste("MSE Before Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# We are going to iterate five times and print the respective plots
for (i in 1:500){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# W vector
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:500){
# Step size
theta <- theta - (0.5)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
}
# Initially constants are set to zero
W <- c(0, 0, 0)
print(W)
print(paste("MSE Before Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# We are going to iterate five times and print the respective plots
for (i in 1:500){
# Step size
W <- W - (0.5)*gradMSE(X_data,y_data,W[1],W[2],W[3])
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
}
print(W)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,W[1],W[2],W[3])))
# W vector
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:500){
# Step size
theta <- theta - (0.5)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
}
gradMSE <- function(X, y, w0, w1, w2) {
dMSE_dw0 <- 0
dMSE_dw1 <- 0
dMSE_dw2 <- 0
m = nrow(X)
# Loop through all the examples in the input data
for (i in 1:m){
x_i1 = X$petal_length[i]
x_i2 = X$petal_width[i]
y_i = y$label[i]
sigma = one_layer_neural_network(w0, w1, w2, x_i1, x_i2)  # Output of neural network for example i
# Append the contributions to the sums for each gradient component
dMSE_dw0 <- dMSE_dw0 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(1)
dMSE_dw1 <- dMSE_dw1 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(x_i1)
dMSE_dw2 <- dMSE_dw2 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(x_i2)
}
# Compile all components into a 3-dimensional vector
gradient_result = c(dMSE_dw0, dMSE_dw1, dMSE_dw2)
# Return result as a vector
return(gradient_result)
}
# W vector
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:500){
# Step size
theta <- theta - (0.5)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
}
# Optimal Error from Optimization Algorithm
paste0("Optimal Mean Squared Error: ", mean_squared_error(X_data, y_data, -4.718, -0.04879, 3.0418))
z_function3 <- function(x) ((4.718/3.0148) + (0.04879/3.0418)*x)
plot4 <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal Mean Square Error: Petal Width vs Petal Length for Versicolor and Virginica") + stat_function(fun = z_function3)
plot4
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:1000){
theta <- theta - (0.5)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(datasets)
iris_data <- read.csv("./irisdata.csv")
# Lets look at the head of the iris data set
head(iris_data)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(datasets)
iris_data <- read.csv("./irisdata.csv")
summary(iris_data)
# Lets look at the head of the iris data set
head(iris_data)
# Number of rows of the iris dataset (number of individual observations)
nrow(iris_data)
# All of the columns are numeric except for the species column
# They are continuous because they correspond to length and width (except for species) # The species are discrete
sapply(iris_data, class)
# The only categorical variable is species in this iris dataset
sapply(iris_data, class)
# Lets pick the 2nd and 3rd classes: versicolor and virginica and let's plot the subset
plot1 <- iris_data %>% filter(species %in% c('versicolor','virginica')) %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Petal Width vs Petal Length for Versicolor and Virginica")
plot1
# inputs: c0, c1, c2, petal_width, petal_length
# outputs: value between 0 and 1 representing the probability
one_layer_neural_network <- function(w0, w1, w2, petal_length, petal_width) {
# Linear function
z <- w0 + w1*petal_length + w2*petal_width
#result
result <- 1/(1 + exp(-z))
return (result)
}
z_function <- function(x) (2.8 + (-1/4)*x)
# Lets pick the 2nd and 3rd classes: versicolor and virginica and let's plot the subset
plot2 <- iris_data %>% filter(species %in% c('versicolor','virginica')) %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Petal Width vs Petal Length for Versicolor and Virginica") + stat_function(fun = z_function)
plot2
# Versicolor
data1 <- iris_data %>% filter (species == 'versicolor')
for (i in 1:50){
if (one_layer_neural_network(-2.8, 0.25, 1, data1$petal_length[i], data1$petal_width[i]) < 0.5){
resulting_flower <- "versicolor"
} else {
resulting_flower <- "virginica"
}
print(paste("Neural network output for example", i,"=",one_layer_neural_network(-2.8, 0.25, 1, data1$petal_length[i], data1$petal_width[i])))
print(paste0("actual: versicolor, predicted: ", resulting_flower))
print(" ")
}
# Virginica
data2 <- iris_data %>% filter (species == 'virginica')
for (j in 1:50){
if (one_layer_neural_network(-2.8, 0.25, 1, data2$petal_length[j], data2$petal_width[j]) < 0.5){
resulting_flower <- "versicolor"
} else {
resulting_flower <- "virginica"
}
print(paste("Neural network output for example", j,"=",one_layer_neural_network(-2.8, 0.25, 1, data2$petal_length[j], data2$petal_width[j])))
print(paste0("actual: virginica, predicted: ", resulting_flower))
print(" ")
}
# Input feature vectors
X_data <- iris_data %>% filter(species %in% c("versicolor","virginica")) %>% select(petal_length, petal_width)
# Pattern classes for each feature vector above
y_data <- iris_data %>%
filter(species %in% c("versicolor","virginica")) %>%
select(species) %>%
mutate(label = ifelse(species == "versicolor", 0, 1)) %>%
select(-species)
mean_squared_error <- function(X, y, w0, w1, w2) {
# Outputs of neural network for each example
NN_outputs <- rep(0,nrow(X))
for (i in 1:nrow(X)){
NN_outputs[i] = one_layer_neural_network(w0, w1, w2, X$petal_length[i], X$petal_width[i])
}
# Compute the differences between the NN outputs and the labels
result = mean((NN_outputs - y$label)^2)
# Return result
return(result)
}
# Mean squared error for decision boundary we guessed above
paste0("Previous Mean Square Error: ", mean_squared_error(X_data, y_data, -2.8, 0.25, 1))
# High error
paste0("High Mean Square Error: ", mean_squared_error(X_data, y_data, 1, -0.25, -1))
z_function2 <- function(x) (1 - (0.25)*x)
plot3 <- iris_data %>% filter(species %in% c('versicolor','virginica')) %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("High Mean Square Error: Petal Width vs Petal Length for Versicolor and Virginica") + stat_function(fun = z_function2)
plot3
# Low error
paste0("Low Mean Square Error: ", mean_squared_error(X_data, y_data, -3.15, 1/3, 1))
z_function3 <- function(x) (3.15 + (-1/3)*x)
plot4 <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Low Mean Square Error: Petal Width vs Petal Length for Versicolor and Virginica") + stat_function(fun = z_function3)
plot4
gradMSE <- function(X, y, w0, w1, w2) {
dMSE_dw0 <- 0
dMSE_dw1 <- 0
dMSE_dw2 <- 0
m = nrow(X)
# Loop through all the examples in the input data
for (i in 1:m){
x_i1 = X$petal_length[i]
x_i2 = X$petal_width[i]
y_i = y$label[i]
sigma = one_layer_neural_network(w0, w1, w2, x_i1, x_i2)  # Output of neural network for example i
# Append the contributions to the sums for each gradient component
dMSE_dw0 <- dMSE_dw0 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(1)
dMSE_dw1 <- dMSE_dw1 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(x_i1)
dMSE_dw2 <- dMSE_dw2 + (2/m)*(sigma - y_i)*(sigma)*(1 - sigma)*(x_i2)
}
# Compile all components into a 3-dimensional vector
gradient_result = c(dMSE_dw0, dMSE_dw1, dMSE_dw2)
# Return result as a vector
return(gradient_result)
}
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:500){
theta <- theta - (0.5)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
}
# Initial theta
theta <- c(0,0,0)
print(theta)
for (i in 1:10000){
theta <- theta - (0.01)*gradMSE(X_data,y_data,theta[1],theta[2],theta[3])
}
print(theta)
print(paste("MSE After Update:",mean_squared_error(X_data,y_data,theta[1],theta[2],theta[3])))
# Optimal Error from Optimization Algorithm
paste0("Optimal Mean Squared Error: ", mean_squared_error(X_data, y_data, -4.718, -0.04879, 3.0418))
z_function3 <- function(x) ((2.5110625/1.9472242) + (0.1235424/1.9472242)*x)
plot4 <- iris_data %>% filter(species == 'versicolor' | species == 'virginica') %>%
ggplot(aes(x = petal_length, y = petal_width, color = species)) + geom_point() +
labs(color = "Species") + xlab("Petal Length") + ylab("Petal Width") +
ggtitle("Optimal Mean Square Error: Petal Width vs Petal Length for Versicolor and Virginica") + stat_function(fun = z_function3)
plot4
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(datasets)
iris_data <- read.csv("./irisdata.csv")
iris_data <- read.csv("./irisdata.csv")
